{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final TP - NLP\n",
    "## Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['120', '265', '620', '599', '261', '567', '230', '431', '188', '201']\n"
     ]
    }
   ],
   "source": [
    "path = \"Tobacco3482-OCR/\"\n",
    "classes = os.listdir(path)\n",
    "\n",
    "nb = []\n",
    "x = []\n",
    "y = []\n",
    "for cls in classes:\n",
    "    files = os.listdir(path + cls)\n",
    "    for file in files:\n",
    "        with open(path + cls + \"/\" + file, 'r') as f:\n",
    "            txt = f.read()\n",
    "        x.append(txt)\n",
    "        y.append(cls)\n",
    "    nb.append(str(len(files)))\n",
    "print(str(nb))\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "#print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace the \\n with space\n",
    "for i in range(x.shape[0]):\n",
    "    x[i] = x[i].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1582., 1145.,  449.,  132.,   66.,   60.,   30.,   11.,    3.,\n",
       "           4.]),\n",
       " array([   0. ,  157.9,  315.8,  473.7,  631.6,  789.5,  947.4, 1105.3,\n",
       "        1263.2, 1421.1, 1579. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of texts' lengthes\n",
    "x_token = []\n",
    "for text in x:\n",
    "    tokens = text.split()\n",
    "    x_token.append(tokens)\n",
    "#print(x[0])\n",
    "#print(x_token[0])\n",
    "lengthes = []\n",
    "for i in range(len(x)):\n",
    "    lengthes.append(len(x_token[i]))\n",
    "plt.hist(lengthes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create document vectors\n",
    "# YOUR CODE HERE\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "vectorizer.fit(x_train)\n",
    "x_train_counts = vectorizer.transform(x_train)\n",
    "x_test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# With TF-IDF representation\n",
    "tf_transformer = TfidfTransformer()\n",
    "tfidf = tf_transformer.fit(x_train_counts)\n",
    "x_train_tf = tfidf.transform(x_train_counts)\n",
    "x_test_tf = tfidf.transform(x_test_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray of NB: 0.6944045911047346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_tf, y_train)\n",
    "res = clf.score(x_test_tf, y_test)\n",
    "print('Accuray of NB: ' + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.73      0.49      0.59        45\n",
      "        Email       0.91      0.95      0.93       128\n",
      "         Form       0.69      0.80      0.74        96\n",
      "       Letter       0.58      0.77      0.66       104\n",
      "         Memo       0.54      0.87      0.66       119\n",
      "         News       0.80      0.53      0.64        30\n",
      "         Note       0.50      0.03      0.05        40\n",
      "       Report       1.00      0.06      0.11        50\n",
      "       Resume       1.00      0.97      0.98        31\n",
      "   Scientific       0.81      0.56      0.66        54\n",
      "\n",
      "    micro avg       0.69      0.69      0.69       697\n",
      "    macro avg       0.76      0.60      0.60       697\n",
      " weighted avg       0.73      0.69      0.66       697\n",
      "\n",
      "[[ 22   2   3   7  10   0   1   0   0   0]\n",
      " [  0 122   0   4   2   0   0   0   0   0]\n",
      " [  1   1  77   5  12   0   0   0   0   0]\n",
      " [  0   1   0  80  22   0   0   0   0   1]\n",
      " [  0   2   2  10 103   0   0   0   0   2]\n",
      " [  4   0   3   5   1  16   0   0   0   1]\n",
      " [  2   5  17   3  12   0   1   0   0   0]\n",
      " [  0   1   3  19  18   3   0   3   0   3]\n",
      " [  0   0   1   0   0   0   0   0  30   0]\n",
      " [  1   0   6   5  11   1   0   0   0  30]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# YOUR CODE HERE\n",
    "y_pred = clf.predict(x_test_tf)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "from nn_utils import TrainingHistory\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN avec représentation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_FEATURES = 10000\n",
    "MAX_TEXT_LENGTH = 2000\n",
    "#EMBED_SIZE  = 300\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_CLASS = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = x_train_tf.toarray()\n",
    "x_train_tf = np.reshape(x_train_tf, (x_train_tf.shape[0], x_train_tf.shape[1], 1))\n",
    "x_test_tf = x_test_tf.toarray()\n",
    "x_test_tf = np.reshape(x_test_tf, (x_test_tf.shape[0], x_test_tf.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Advertisement' 'Email' 'Form' 'Letter' 'Memo' 'News' 'Note' 'Report'\n",
      " 'Resume' 'Scientific']\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2000, 64)          192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              65537024  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 65,547,466\n",
      "Trainable params: 65,547,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2506 samples, validate on 279 samples\n",
      "Epoch 1/20\n",
      "2506/2506 [==============================] - 173s 69ms/step - loss: 1.3451 - acc: 0.5599 - val_loss: 0.8938 - val_acc: 0.6918\n",
      "Epoch 2/20\n",
      "2506/2506 [==============================] - 186s 74ms/step - loss: 0.7612 - acc: 0.7506 - val_loss: 0.8588 - val_acc: 0.7025\n",
      "Epoch 3/20\n",
      "2506/2506 [==============================] - 180s 72ms/step - loss: 0.6048 - acc: 0.7941 - val_loss: 0.7912 - val_acc: 0.7276\n",
      "Epoch 4/20\n",
      "2506/2506 [==============================] - 181s 72ms/step - loss: 0.5210 - acc: 0.8272 - val_loss: 0.7951 - val_acc: 0.7312\n",
      "Epoch 5/20\n",
      "2506/2506 [==============================] - 182s 73ms/step - loss: 0.4728 - acc: 0.8328 - val_loss: 0.8553 - val_acc: 0.7204\n",
      "Epoch 6/20\n",
      "2506/2506 [==============================] - 155s 62ms/step - loss: 0.4297 - acc: 0.8500 - val_loss: 0.8323 - val_acc: 0.7168\n",
      "Epoch 7/20\n",
      "2506/2506 [==============================] - 153s 61ms/step - loss: 0.3441 - acc: 0.8787 - val_loss: 0.7786 - val_acc: 0.7527\n",
      "Epoch 8/20\n",
      "2506/2506 [==============================] - 144s 57ms/step - loss: 0.3245 - acc: 0.8791 - val_loss: 0.8032 - val_acc: 0.7276\n",
      "Epoch 9/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.3226 - acc: 0.8875 - val_loss: 0.7956 - val_acc: 0.7348\n",
      "Epoch 10/20\n",
      "2506/2506 [==============================] - 138s 55ms/step - loss: 0.2644 - acc: 0.9066 - val_loss: 0.8448 - val_acc: 0.7133\n",
      "Epoch 11/20\n",
      "2506/2506 [==============================] - 138s 55ms/step - loss: 0.2681 - acc: 0.8994 - val_loss: 0.8259 - val_acc: 0.7599\n",
      "Epoch 12/20\n",
      "2506/2506 [==============================] - 138s 55ms/step - loss: 0.2453 - acc: 0.9110 - val_loss: 0.8374 - val_acc: 0.7491\n",
      "Epoch 13/20\n",
      "2506/2506 [==============================] - 138s 55ms/step - loss: 0.2186 - acc: 0.9226 - val_loss: 0.8907 - val_acc: 0.7455\n",
      "Epoch 14/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.1984 - acc: 0.9318 - val_loss: 0.9754 - val_acc: 0.7348\n",
      "Epoch 15/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.2030 - acc: 0.9270 - val_loss: 0.9115 - val_acc: 0.7240\n",
      "Epoch 16/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.2002 - acc: 0.9385 - val_loss: 0.9064 - val_acc: 0.6953\n",
      "Epoch 17/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.2017 - acc: 0.9342 - val_loss: 0.8913 - val_acc: 0.7168\n",
      "Epoch 18/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.1907 - acc: 0.9358 - val_loss: 0.8929 - val_acc: 0.7312\n",
      "Epoch 19/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.1721 - acc: 0.9346 - val_loss: 0.9863 - val_acc: 0.7204\n",
      "Epoch 20/20\n",
      "2506/2506 [==============================] - 139s 55ms/step - loss: 0.1784 - acc: 0.9397 - val_loss: 0.9736 - val_acc: 0.7133\n",
      "Test Accuracy: 0.7747489239598279\n",
      "p r f1 77.5 77.47 77.475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.71      0.74        45\n",
      "           1       0.96      0.93      0.94       128\n",
      "           2       0.86      0.79      0.83        96\n",
      "           3       0.68      0.83      0.75       104\n",
      "           4       0.77      0.83      0.80       119\n",
      "           5       0.54      0.83      0.66        30\n",
      "           6       0.67      0.65      0.66        40\n",
      "           7       0.59      0.20      0.30        50\n",
      "           8       1.00      0.97      0.98        31\n",
      "           9       0.65      0.69      0.67        54\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       697\n",
      "   macro avg       0.75      0.74      0.73       697\n",
      "weighted avg       0.78      0.77      0.77       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,1))\n",
    "    #model = Embedding(MAX_FEATURES, EMBED_SIZE)(inp)\n",
    "    model = Dropout(0.5)(inp)\n",
    "    model = Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fit_predict(model, x_train, x_test, y, history):\n",
    "    \n",
    "    model.fit(x_train, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1,\n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    return model.predict(x_test)\n",
    "\n",
    "\n",
    "# Get the list of different classes\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "n_out = len(CLASSES_LIST)\n",
    "print(CLASSES_LIST)\n",
    "\n",
    "# Convert clas string to index\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CLASSES_LIST)\n",
    "y_train = le.transform(y_train) \n",
    "y_test = le.transform(y_test) \n",
    "train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "\n",
    "# get the textual data in the correct format for NN\n",
    "#x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "#print(len(x_vec_train), len(x_vec_test))\n",
    "\n",
    "# define the NN topology\n",
    "model = get_model()\n",
    "\n",
    "# Define training procedure\n",
    "history = TrainingHistory(x_test_tf, y_test, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_train_tf, x_test_tf, train_y_cat, history).argmax(1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN une matrice d’embedding basée sur TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create document vectors\n",
    "# YOUR CODE HERE\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "vectorizer.fit(x_train)\n",
    "x_train_counts = vectorizer.transform(x_train)\n",
    "x_test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# With TF-IDF representation\n",
    "tf_transformer = TfidfTransformer()\n",
    "tfidf = tf_transformer.fit(x_train_counts)\n",
    "x_train_tf = tfidf.transform(x_train_counts)\n",
    "x_test_tf = tfidf.transform(x_test_counts)\n",
    "\n",
    "# Use x_train_tf as the embedding matrix\n",
    "embedding_matrix = x_train_tf.toarray().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "def get_model_1(embedding_matrix):\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_WORDS,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 2000, 2785)        27850000  \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 2000, 2785)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 2000, 32)          178272    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              32769024  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 60,807,546\n",
      "Trainable params: 32,957,546\n",
      "Non-trainable params: 27,850,000\n",
      "_________________________________________________________________\n",
      "Train on 2506 samples, validate on 279 samples\n",
      "Epoch 1/10\n",
      "2506/2506 [==============================] - 260s 104ms/step - loss: 2.2314 - acc: 0.1644 - val_loss: 2.1911 - val_acc: 0.1828\n",
      "Epoch 2/10\n",
      "2506/2506 [==============================] - 258s 103ms/step - loss: 2.1410 - acc: 0.2191 - val_loss: 2.2096 - val_acc: 0.1792\n",
      "Epoch 3/10\n",
      "2506/2506 [==============================] - 256s 102ms/step - loss: 1.9265 - acc: 0.3300 - val_loss: 2.3320 - val_acc: 0.1577\n",
      "Epoch 4/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 1.5291 - acc: 0.4697 - val_loss: 2.5180 - val_acc: 0.1828\n",
      "Epoch 5/10\n",
      "2506/2506 [==============================] - 256s 102ms/step - loss: 1.1978 - acc: 0.5778 - val_loss: 2.7335 - val_acc: 0.1541\n",
      "Epoch 6/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 0.9969 - acc: 0.6584 - val_loss: 2.9465 - val_acc: 0.1685\n",
      "Epoch 7/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 0.8339 - acc: 0.7179 - val_loss: 3.1916 - val_acc: 0.1792\n",
      "Epoch 8/10\n",
      "2506/2506 [==============================] - 257s 103ms/step - loss: 0.7339 - acc: 0.7518 - val_loss: 3.2947 - val_acc: 0.1792\n",
      "Epoch 9/10\n",
      "2506/2506 [==============================] - 257s 103ms/step - loss: 0.6524 - acc: 0.7777 - val_loss: 3.4077 - val_acc: 0.1649\n",
      "Epoch 10/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 0.5420 - acc: 0.8192 - val_loss: 3.5760 - val_acc: 0.1720\n",
      "Test Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yibing/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:182: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  score = y_true == y_pred\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-04f6374dd239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                               \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                               labels=[x for x in \n\u001b[0;32m---> 19\u001b[0;31m                                                       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                                                       if x not in ['CSDECMOTV']])\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = len(x_train)\n",
    "MAX_WORDS = 10000\n",
    "\n",
    "x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "model = get_model_1(embedding_matrix)\n",
    "\n",
    "# Define training procedure\n",
    "history = TrainingHistory(x_vec_test, y_test, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat, history).argmax(1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with embedding matrix given by word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all the texts into one file\n",
    "path = \"Tobacco3482-OCR/\"\n",
    "classes = os.listdir(path)\n",
    "\n",
    "text = open('text_pure_nolabel.txt', 'a')\n",
    "for cls in classes:\n",
    "    files = os.listdir(path + cls)\n",
    "    for file in files:\n",
    "        with open(path + cls + \"/\" + file, 'r') as f:\n",
    "            txt = f.read()\n",
    "            txt = txt.replace(\"\\n\", \" \")\n",
    "            text.writelines((txt,'\\n'))\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-fb79bc46a04e>\", line 5, in <module>\n",
      "    model_word2vec = Word2Vec(LineSentence(file_path), workers=4, sg=1, size=150, iter=100)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\", line 767, in __init__\n",
      "    fast_version=FAST_VERSION)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\", line 763, in __init__\n",
      "    end_alpha=self.min_alpha, compute_loss=compute_loss)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\", line 892, in train\n",
      "    queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\", line 1081, in train\n",
      "    **kwargs)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\", line 553, in train\n",
      "    total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\", line 489, in _train_epoch\n",
      "    report_delay=report_delay, is_corpus_file_mode=False)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\", line 346, in _log_epoch_progress\n",
      "    report = progress_queue.get()  # blocks if workers too slow\n",
      "  File \"/usr/lib/python3.6/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/yibing/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 378, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/usr/lib/python3.6/posixpath.py\", line 365, in normpath\n",
      "    path = sep*initial_slashes + path\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "file_path = 'text_pure_nolabel.txt'\n",
    "\n",
    "# Skipgram model\n",
    "#tic = time.time()\n",
    "model_word2vec = Word2Vec(LineSentence(file_path), workers=4, sg=1, size=150, iter=100)\n",
    "#toc = time.time()\n",
    "#print(\"Time used to train a word2vec_skipgram word representation model: \" + str(toc - tic) + \"s\")\n",
    "\n",
    "# Sauvegarder le modèle et les vecteurs entraînés\n",
    "model_word2vec.save('word2vec_skip.model')\n",
    "model_word2vec.wv.save_word2vec_format('word2vec_skip.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_TEXT_LENGTH = 500\n",
    "MAX_WORDS = 10000\n",
    "EMBED_SIZE = 150\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_CLASS = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_index(vectors_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(vectors_file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        #print(first_line)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_embedding_matrix(word_index, embedding_index):\n",
    "    print('Building embedding matrix...')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('Embedding matrix built.')        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    word_index = tokenizer.word_index\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           word_index\n",
    "\n",
    "\n",
    "def get_model_2(embedding_matrix, word_index, print_sum=True):\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "\n",
    "    model = Embedding(len(word_index) + 1,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    \n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if print_sum:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'Tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8d46ca7a32ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_vec_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vec_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvectors_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'word2vec_skip.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-bd23518cfb39>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m(train_raw_text, test_raw_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_raw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_raw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_raw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'Tokenizer'"
     ]
    }
   ],
   "source": [
    "\n",
    "x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test)\n",
    "\n",
    "vectors_file_path = 'word2vec_skip.txt'\n",
    "embedding_index = get_embedding_index(vectors_file_path)\n",
    "embedding_matrix = get_embedding_matrix(word_index, embedding_index)\n",
    "print('Building model...')\n",
    "\n",
    "#x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "model = get_model_2(embedding_matrix, word_index)\n",
    "\n",
    "history = TrainingHistory(x_vec_test, y_test, CLASSES_LIST)\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat, history).argmax(1)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
