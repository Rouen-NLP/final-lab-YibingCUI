{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final TP - NLP\n",
    "## Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['120', '265', '620', '599', '261', '567', '230', '431', '188', '201']\n"
     ]
    }
   ],
   "source": [
    "path = \"Tobacco3482-OCR/\"\n",
    "classes = os.listdir(path)\n",
    "\n",
    "nb = []\n",
    "x = []\n",
    "y = []\n",
    "for cls in classes:\n",
    "    files = os.listdir(path + cls)\n",
    "    for file in files:\n",
    "        with open(path + cls + \"/\" + file, 'r') as f:\n",
    "            txt = f.read()\n",
    "        x.append(txt)\n",
    "        y.append(cls)\n",
    "    nb.append(str(len(files)))\n",
    "print(str(nb))\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse du problème \n",
    "Le problème que nous sommes censés résoudre est de fournir une solution permettant de classer les documents en plusieurs classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace the \\n with space\n",
    "for i in range(x.shape[0]):\n",
    "    x[i] = x[i].replace(\"\\n\", \" \")\n",
    "# print(x)\n",
    "x_token = []\n",
    "for text in x:\n",
    "    tokens = text.split()\n",
    "    x_token.append(tokens)\n",
    "#print(x[0])\n",
    "#print(x_token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_token, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create document vectors\n",
    "# YOUR CODE HERE\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "vectorizer.fit(x_train)\n",
    "x_train_counts = vectorizer.transform(x_train)\n",
    "x_test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# With TF-IDF representation\n",
    "tf_transformer = TfidfTransformer()\n",
    "tfidf = tf_transformer.fit(x_train_counts)\n",
    "x_train_tf = tfidf.transform(x_train_counts)\n",
    "x_test_tf = tfidf.transform(x_test_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray of NB: 0.6857962697274032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_tf, y_train)\n",
    "res = clf.score(x_test_tf, y_test)\n",
    "print('Accuray of NB: ' + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.59      0.39      0.47        33\n",
      "        Email       0.85      0.95      0.90       110\n",
      "         Form       0.65      0.81      0.72        91\n",
      "       Letter       0.57      0.79      0.66       109\n",
      "         Memo       0.60      0.77      0.67       141\n",
      "         News       0.91      0.64      0.75        33\n",
      "         Note       1.00      0.02      0.04        49\n",
      "       Report       0.73      0.16      0.26        50\n",
      "       Resume       0.96      1.00      0.98        23\n",
      "   Scientific       0.80      0.67      0.73        58\n",
      "\n",
      "    micro avg       0.69      0.69      0.69       697\n",
      "    macro avg       0.77      0.62      0.62       697\n",
      " weighted avg       0.72      0.69      0.65       697\n",
      "\n",
      "[[ 13   2   8   4   5   1   0   0   0   0]\n",
      " [  0 105   0   3   2   0   0   0   0   0]\n",
      " [  2   0  74   4   9   0   0   0   0   2]\n",
      " [  0   1   0  86  20   0   0   0   0   2]\n",
      " [  1   6   3  23 108   0   0   0   0   0]\n",
      " [  1   0   1   7   2  21   0   1   0   0]\n",
      " [  5   9  20   5   8   1   1   0   0   0]\n",
      " [  0   0   1  16  18   0   0   8   1   6]\n",
      " [  0   0   0   0   0   0   0   0  23   0]\n",
      " [  0   0   7   2   8   0   0   2   0  39]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# YOUR CODE HERE\n",
    "y_pred = clf.predict(x_test_tf)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from nn_utils import TrainingHistory\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_FEATURES = 10000\n",
    "MAX_TEXT_LENGTH = 2000\n",
    "#EMBED_SIZE  = 300\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_CLASS = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = x_train_tf.toarray()\n",
    "x_train_tf = np.reshape(x_train_tf, (x_train_tf.shape[0], x_train_tf.shape[1], 1))\n",
    "x_test_tf = np.reshape(x_test_tf.toarray(), (x_test_tf.shape[0], x_test_tf.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 2000, 32)          96        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              32769024  \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 32,779,370\n",
      "Trainable params: 32,779,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2506 samples, validate on 279 samples\n",
      "Epoch 1/10\n",
      "2506/2506 [==============================] - 73s 29ms/step - loss: 1.3386 - acc: 0.5583 - val_loss: 0.8386 - val_acc: 0.7061\n",
      "Epoch 2/10\n",
      "2506/2506 [==============================] - 70s 28ms/step - loss: 0.7371 - acc: 0.7534 - val_loss: 0.7557 - val_acc: 0.7348\n",
      "Epoch 3/10\n",
      "2506/2506 [==============================] - 70s 28ms/step - loss: 0.5738 - acc: 0.8077 - val_loss: 0.8031 - val_acc: 0.7276\n",
      "Epoch 4/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.4943 - acc: 0.8372 - val_loss: 0.7852 - val_acc: 0.7419\n",
      "Epoch 5/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.4345 - acc: 0.8516 - val_loss: 0.8066 - val_acc: 0.7348\n",
      "Epoch 6/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.3844 - acc: 0.8647 - val_loss: 0.8274 - val_acc: 0.7133\n",
      "Epoch 7/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.3468 - acc: 0.8763 - val_loss: 0.8153 - val_acc: 0.7312\n",
      "Epoch 8/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.3181 - acc: 0.8871 - val_loss: 0.8623 - val_acc: 0.7133\n",
      "Epoch 9/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.3134 - acc: 0.8879 - val_loss: 0.8459 - val_acc: 0.7384\n",
      "Epoch 10/10\n",
      "2506/2506 [==============================] - 71s 28ms/step - loss: 0.2612 - acc: 0.9094 - val_loss: 0.8643 - val_acc: 0.7276\n",
      "Test Accuracy: 0.7690100430416069\n",
      "p r f1 76.9 76.90 76.901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.58      0.62        33\n",
      "           1       0.99      0.94      0.96       110\n",
      "           2       0.75      0.86      0.80        91\n",
      "           3       0.69      0.79      0.74       109\n",
      "           4       0.83      0.79      0.81       141\n",
      "           5       0.66      0.88      0.75        33\n",
      "           6       0.73      0.45      0.56        49\n",
      "           7       0.46      0.44      0.45        50\n",
      "           8       0.96      1.00      0.98        23\n",
      "           9       0.75      0.74      0.75        58\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       697\n",
      "   macro avg       0.75      0.75      0.74       697\n",
      "weighted avg       0.77      0.77      0.77       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,1))\n",
    "    #model = Embedding(MAX_FEATURES, EMBED_SIZE)(inp)\n",
    "    model = Dropout(0.5)(inp)\n",
    "    model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fit_predict(model, x_train, x_test, y, history):\n",
    "    \n",
    "    model.fit(x_train, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1,\n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    return model.predict(x_test)\n",
    "\n",
    "\n",
    "# Get the list of different classes\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "n_out = len(CLASSES_LIST)\n",
    "print(CLASSES_LIST)\n",
    "\n",
    "# Convert clas string to index\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CLASSES_LIST)\n",
    "y_train = le.transform(y_train) \n",
    "y_test = le.transform(y_test) \n",
    "train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "\n",
    "# get the textual data in the correct format for NN\n",
    "#x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "#print(len(x_vec_train), len(x_vec_test))\n",
    "\n",
    "# define the NN topology\n",
    "model = get_model()\n",
    "\n",
    "# Define training procedure\n",
    "history = TrainingHistory(x_test_tf, y_test, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_train_tf, x_test_tf, train_y_cat, history).argmax(1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "def get_model_1():\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_FEATURES + 1,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    model = Dropout(0.5)(inp)\n",
    "    model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = len(x_train)\n",
    "\n",
    "x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "model = get_model()\n",
    "\n",
    "# Define training procedure\n",
    "history = TrainingHistory(x_vec_test, y_test, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat, history).argmax(1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
