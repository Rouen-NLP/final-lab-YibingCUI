{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final TP - NLP\n",
    "## Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['120', '265', '620', '599', '261', '567', '230', '431', '188', '201']\n"
     ]
    }
   ],
   "source": [
    "path = \"Tobacco3482-OCR/\"\n",
    "classes = os.listdir(path)\n",
    "\n",
    "nb = []\n",
    "x = []\n",
    "y = []\n",
    "for cls in classes:\n",
    "    files = os.listdir(path + cls)\n",
    "    for file in files:\n",
    "        with open(path + cls + \"/\" + file, 'r') as f:\n",
    "            txt = f.read()\n",
    "        x.append(txt)\n",
    "        y.append(cls)\n",
    "    nb.append(str(len(files)))\n",
    "print(str(nb))\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "#print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace the \\n with space\n",
    "for i in range(x.shape[0]):\n",
    "    x[i] = x[i].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1582., 1145.,  449.,  132.,   66.,   60.,   30.,   11.,    3.,\n",
       "           4.]),\n",
       " array([   0. ,  157.9,  315.8,  473.7,  631.6,  789.5,  947.4, 1105.3,\n",
       "        1263.2, 1421.1, 1579. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of texts' lengthes\n",
    "x_token = []\n",
    "for text in x:\n",
    "    tokens = text.split()\n",
    "    x_token.append(tokens)\n",
    "#print(x[0])\n",
    "#print(x_token[0])\n",
    "lengthes = []\n",
    "for i in range(len(x)):\n",
    "    lengthes.append(len(x_token[i]))\n",
    "plt.hist(lengthes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create document vectors\n",
    "# YOUR CODE HERE\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "vectorizer.fit(x_train)\n",
    "x_train_counts = vectorizer.transform(x_train)\n",
    "x_test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# With TF-IDF representation\n",
    "tf_transformer = TfidfTransformer()\n",
    "tfidf = tf_transformer.fit(x_train_counts)\n",
    "x_train_tf = tfidf.transform(x_train_counts)\n",
    "x_test_tf = tfidf.transform(x_test_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray of NB: 0.6857962697274032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_tf, y_train)\n",
    "res = clf.score(x_test_tf, y_test)\n",
    "print('Accuray of NB: ' + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.59      0.39      0.47        33\n",
      "        Email       0.85      0.95      0.90       110\n",
      "         Form       0.65      0.81      0.72        91\n",
      "       Letter       0.57      0.79      0.66       109\n",
      "         Memo       0.60      0.77      0.67       141\n",
      "         News       0.91      0.64      0.75        33\n",
      "         Note       1.00      0.02      0.04        49\n",
      "       Report       0.73      0.16      0.26        50\n",
      "       Resume       0.96      1.00      0.98        23\n",
      "   Scientific       0.80      0.67      0.73        58\n",
      "\n",
      "    micro avg       0.69      0.69      0.69       697\n",
      "    macro avg       0.77      0.62      0.62       697\n",
      " weighted avg       0.72      0.69      0.65       697\n",
      "\n",
      "[[ 13   2   8   4   5   1   0   0   0   0]\n",
      " [  0 105   0   3   2   0   0   0   0   0]\n",
      " [  2   0  74   4   9   0   0   0   0   2]\n",
      " [  0   1   0  86  20   0   0   0   0   2]\n",
      " [  1   6   3  23 108   0   0   0   0   0]\n",
      " [  1   0   1   7   2  21   0   1   0   0]\n",
      " [  5   9  20   5   8   1   1   0   0   0]\n",
      " [  0   0   1  16  18   0   0   8   1   6]\n",
      " [  0   0   0   0   0   0   0   0  23   0]\n",
      " [  0   0   7   2   8   0   0   2   0  39]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# YOUR CODE HERE\n",
    "y_pred = clf.predict(x_test_tf)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from nn_utils import TrainingHistory\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_FEATURES = 10000\n",
    "MAX_TEXT_LENGTH = 2000\n",
    "#EMBED_SIZE  = 300\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_CLASS = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-3c30476f8779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_test_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_test_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "x_train_tf = x_train_tf.toarray()\n",
    "x_train_tf = np.reshape(x_train_tf, (x_train_tf.shape[0], x_train_tf.shape[1], 1))\n",
    "x_test_tf = x_test_tf.toarray()\n",
    "x_test_tf = np.reshape(x_test_tf, (x_test_tf.shape[0], x_test_tf.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Advertisement' 'Email' 'Form' 'Letter' 'Memo' 'News' 'Note' 'Report'\n",
      " 'Resume' 'Scientific']\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 2000, 64)          192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1024)              65537024  \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 65,547,466\n",
      "Trainable params: 65,547,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2506 samples, validate on 279 samples\n",
      "Epoch 1/10\n",
      "2506/2506 [==============================] - 157s 63ms/step - loss: 1.3249 - acc: 0.5607 - val_loss: 0.7581 - val_acc: 0.7133\n",
      "Epoch 2/10\n",
      "2506/2506 [==============================] - 147s 59ms/step - loss: 0.7799 - acc: 0.7406 - val_loss: 0.6618 - val_acc: 0.7491\n",
      "Epoch 3/10\n",
      "2506/2506 [==============================] - 144s 58ms/step - loss: 0.6085 - acc: 0.7933 - val_loss: 0.6723 - val_acc: 0.7455\n",
      "Epoch 4/10\n",
      "2506/2506 [==============================] - 146s 58ms/step - loss: 0.5113 - acc: 0.8292 - val_loss: 0.6826 - val_acc: 0.7491\n",
      "Epoch 5/10\n",
      "2506/2506 [==============================] - 137s 55ms/step - loss: 0.4762 - acc: 0.8448 - val_loss: 0.6689 - val_acc: 0.7599\n",
      "Epoch 6/10\n",
      "2506/2506 [==============================] - 137s 55ms/step - loss: 0.4333 - acc: 0.8559 - val_loss: 0.6827 - val_acc: 0.7634\n",
      "Epoch 7/10\n",
      "2506/2506 [==============================] - 137s 55ms/step - loss: 0.3463 - acc: 0.8815 - val_loss: 0.6912 - val_acc: 0.7634\n",
      "Epoch 8/10\n",
      "2506/2506 [==============================] - 137s 55ms/step - loss: 0.3203 - acc: 0.8823 - val_loss: 0.7067 - val_acc: 0.7599\n",
      "Epoch 9/10\n",
      "2506/2506 [==============================] - 138s 55ms/step - loss: 0.3014 - acc: 0.8927 - val_loss: 0.6548 - val_acc: 0.7634\n",
      "Epoch 10/10\n",
      "2506/2506 [==============================] - 137s 55ms/step - loss: 0.2610 - acc: 0.9062 - val_loss: 0.7365 - val_acc: 0.7491\n",
      "Test Accuracy: 0.7790530846484935\n",
      "p r f1 77.9 77.91 77.905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.77        56\n",
      "           1       0.85      0.95      0.90       111\n",
      "           2       0.79      0.84      0.81        96\n",
      "           3       0.75      0.79      0.77       108\n",
      "           4       0.77      0.78      0.78       133\n",
      "           5       0.75      0.92      0.83        36\n",
      "           6       0.65      0.53      0.59        32\n",
      "           7       0.70      0.43      0.53        54\n",
      "           8       1.00      0.95      0.98        22\n",
      "           9       0.68      0.65      0.67        49\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       697\n",
      "   macro avg       0.77      0.76      0.76       697\n",
      "weighted avg       0.77      0.78      0.77       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,1))\n",
    "    #model = Embedding(MAX_FEATURES, EMBED_SIZE)(inp)\n",
    "    model = Dropout(0.5)(inp)\n",
    "    model = Conv1D(filters=64, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fit_predict(model, x_train, x_test, y, history):\n",
    "    \n",
    "    model.fit(x_train, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1,\n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    return model.predict(x_test)\n",
    "\n",
    "\n",
    "# Get the list of different classes\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "n_out = len(CLASSES_LIST)\n",
    "print(CLASSES_LIST)\n",
    "\n",
    "# Convert clas string to index\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CLASSES_LIST)\n",
    "y_train = le.transform(y_train) \n",
    "y_test = le.transform(y_test) \n",
    "train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "\n",
    "# get the textual data in the correct format for NN\n",
    "#x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "#print(len(x_vec_train), len(x_vec_test))\n",
    "\n",
    "# define the NN topology\n",
    "model = get_model()\n",
    "\n",
    "# Define training procedure\n",
    "history = TrainingHistory(x_test_tf, y_test, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_train_tf, x_test_tf, train_y_cat, history).argmax(1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create document vectors\n",
    "# YOUR CODE HERE\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "vectorizer.fit(x_train)\n",
    "x_train_counts = vectorizer.transform(x_train)\n",
    "x_test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# With TF-IDF representation\n",
    "tf_transformer = TfidfTransformer()\n",
    "tfidf = tf_transformer.fit(x_train_counts)\n",
    "x_train_tf = tfidf.transform(x_train_counts)\n",
    "x_test_tf = tfidf.transform(x_test_counts)\n",
    "\n",
    "# Use x_train_tf as the embedding matrix\n",
    "embedding_matrix = x_train_tf.toarray().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "def get_model_1(embedding_matrix):\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_WORDS,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 2000, 2785)        27850000  \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 2000, 2785)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 2000, 32)          178272    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              32769024  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 60,807,546\n",
      "Trainable params: 32,957,546\n",
      "Non-trainable params: 27,850,000\n",
      "_________________________________________________________________\n",
      "Train on 2506 samples, validate on 279 samples\n",
      "Epoch 1/10\n",
      "2506/2506 [==============================] - 260s 104ms/step - loss: 2.2314 - acc: 0.1644 - val_loss: 2.1911 - val_acc: 0.1828\n",
      "Epoch 2/10\n",
      "2506/2506 [==============================] - 258s 103ms/step - loss: 2.1410 - acc: 0.2191 - val_loss: 2.2096 - val_acc: 0.1792\n",
      "Epoch 3/10\n",
      "2506/2506 [==============================] - 256s 102ms/step - loss: 1.9265 - acc: 0.3300 - val_loss: 2.3320 - val_acc: 0.1577\n",
      "Epoch 4/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 1.5291 - acc: 0.4697 - val_loss: 2.5180 - val_acc: 0.1828\n",
      "Epoch 5/10\n",
      "2506/2506 [==============================] - 256s 102ms/step - loss: 1.1978 - acc: 0.5778 - val_loss: 2.7335 - val_acc: 0.1541\n",
      "Epoch 6/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 0.9969 - acc: 0.6584 - val_loss: 2.9465 - val_acc: 0.1685\n",
      "Epoch 7/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 0.8339 - acc: 0.7179 - val_loss: 3.1916 - val_acc: 0.1792\n",
      "Epoch 8/10\n",
      "2506/2506 [==============================] - 257s 103ms/step - loss: 0.7339 - acc: 0.7518 - val_loss: 3.2947 - val_acc: 0.1792\n",
      "Epoch 9/10\n",
      "2506/2506 [==============================] - 257s 103ms/step - loss: 0.6524 - acc: 0.7777 - val_loss: 3.4077 - val_acc: 0.1649\n",
      "Epoch 10/10\n",
      "2506/2506 [==============================] - 257s 102ms/step - loss: 0.5420 - acc: 0.8192 - val_loss: 3.5760 - val_acc: 0.1720\n",
      "Test Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yibing/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:182: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  score = y_true == y_pred\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-04f6374dd239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                               \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                               labels=[x for x in \n\u001b[0;32m---> 19\u001b[0;31m                                                       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                                                       if x not in ['CSDECMOTV']])\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = len(x_train)\n",
    "MAX_WORDS = 10000\n",
    "\n",
    "x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "model = get_model_1(embedding_matrix)\n",
    "\n",
    "# Define training procedure\n",
    "history = TrainingHistory(x_vec_test, y_test, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat, history).argmax(1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with embedding matrix given by word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all the texts into one file\n",
    "path = \"Tobacco3482-OCR/\"\n",
    "classes = os.listdir(path)\n",
    "\n",
    "text = open('text_pure_nolabel.txt', 'a')\n",
    "for cls in classes:\n",
    "    files = os.listdir(path + cls)\n",
    "    for file in files:\n",
    "        with open(path + cls + \"/\" + file, 'r') as f:\n",
    "            txt = f.read()\n",
    "            txt = txt.replace(\"\\n\", \" \")\n",
    "            text.writelines((txt,'\\n'))\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-fb79bc46a04e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Skipgram model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tic = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#toc = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(\"Time used to train a word2vec_skipgram word representation model: \" + str(toc - tic) + \"s\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = 'text_pure_nolabel.txt'\n",
    "\n",
    "# Skipgram model\n",
    "#tic = time.time()\n",
    "model_word2vec = Word2Vec(LineSentence(file_path), workers=4, sg=1, size=150, iter=100)\n",
    "#toc = time.time()\n",
    "#print(\"Time used to train a word2vec_skipgram word representation model: \" + str(toc - tic) + \"s\")\n",
    "\n",
    "# Sauvegarder le modèle et les vecteurs entraînés\n",
    "model_word2vec.save('word2vec_skip.model')\n",
    "model_word2vec.wv.save_word2vec_format('word2vec_skip.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_index(vectors_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(vectors_file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        #print(first_line)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_embedding_matrix(word_index, embedding_index):\n",
    "    print('Building embedding matrix...')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('Embedding matrix built.')        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_TEXT_LENGTH = 500\n",
    "MAX_WORDS = 10000\n",
    "EMBED_SIZE = 150\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1\n",
    "NB_CLASS = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    word_index = tokenizer.word_index\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           word_index\n",
    "\n",
    "\n",
    "def get_model_2(embedding_matrix, word_index, print_sum=True):\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "\n",
    "    model = Embedding(len(word_index) + 1,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    \n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(NB_CLASS, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if print_sum:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'Tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-6844d6ba3a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_vec_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vec_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectors_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'word2vec_skip.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-bd23518cfb39>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m(train_raw_text, test_raw_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_raw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_raw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_raw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'Tokenizer'"
     ]
    }
   ],
   "source": [
    "x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test)\n",
    "\n",
    "vectors_file_path = 'word2vec_skip.txt'\n",
    "embedding_index = get_embedding_index(vectors_file_path)\n",
    "embedding_matrix = get_embedding_matrix(word_index, embedding_index)\n",
    "print('Building model...')\n",
    "\n",
    "#x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "model = get_model_2(embedding_matrix, word_index)\n",
    "\n",
    "history = TrainingHistory(x_vec_test, y_test, CLASSES_LIST)\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat, history).argmax(1)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_predicted, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
